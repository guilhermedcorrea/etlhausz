[2022-11-06T17:36:55.849-0300] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: get_urls_deca.get_produtos scheduled__2022-11-06T20:24:17.733814+00:00 [queued]>
[2022-11-06T17:36:55.893-0300] {taskinstance.py:1165} INFO - Dependencies all met for <TaskInstance: get_urls_deca.get_produtos scheduled__2022-11-06T20:24:17.733814+00:00 [queued]>
[2022-11-06T17:36:55.894-0300] {taskinstance.py:1362} INFO - 
--------------------------------------------------------------------------------
[2022-11-06T17:36:55.894-0300] {taskinstance.py:1363} INFO - Starting attempt 1 of 1
[2022-11-06T17:36:55.894-0300] {taskinstance.py:1364} INFO - 
--------------------------------------------------------------------------------
[2022-11-06T17:36:55.973-0300] {taskinstance.py:1383} INFO - Executing <Task(PythonOperator): get_produtos> on 2022-11-06 20:24:17.733814+00:00
[2022-11-06T17:36:55.982-0300] {standard_task_runner.py:55} INFO - Started process 3698 to run task
[2022-11-06T17:36:56.003-0300] {standard_task_runner.py:82} INFO - Running: ['airflow', 'tasks', 'run', 'get_urls_deca', 'get_produtos', 'scheduled__2022-11-06T20:24:17.733814+00:00', '--job-id', '420', '--raw', '--subdir', 'DAGS_FOLDER/teste_excom.py', '--cfg-path', '/tmp/tmpmt1e1ui0']
[2022-11-06T17:36:56.004-0300] {standard_task_runner.py:83} INFO - Job 420: Subtask get_produtos
[2022-11-06T17:36:56.393-0300] {task_command.py:376} INFO - Running <TaskInstance: get_urls_deca.get_produtos scheduled__2022-11-06T20:24:17.733814+00:00 [running]> on host debian
[2022-11-06T17:36:56.686-0300] {taskinstance.py:1590} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=get_urls_deca
AIRFLOW_CTX_TASK_ID=get_produtos
AIRFLOW_CTX_EXECUTION_DATE=2022-11-06T20:24:17.733814+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-11-06T20:24:17.733814+00:00
[2022-11-06T17:36:56.810-0300] {logging_mixin.py:120} WARNING - /home/debian/etlhausz/venv/lib/python3.9/site-packages/airflow/utils/context.py:297 AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
[2022-11-06T17:36:56.827-0300] {logging_mixin.py:120} INFO - SELECT xcom.dag_run_id AS xcom_dag_run_id, xcom.task_id AS xcom_task_id, xcom.map_index AS xcom_map_index, xcom.key AS xcom_key, xcom.dag_id AS xcom_dag_id, xcom.run_id AS xcom_run_id, xcom.value AS xcom_value, xcom.timestamp AS xcom_timestamp, dag_run_1.state AS dag_run_1_state, dag_run_1.id AS dag_run_1_id, dag_run_1.dag_id AS dag_run_1_dag_id, dag_run_1.queued_at AS dag_run_1_queued_at, dag_run_1.execution_date AS dag_run_1_execution_date, dag_run_1.start_date AS dag_run_1_start_date, dag_run_1.end_date AS dag_run_1_end_date, dag_run_1.run_id AS dag_run_1_run_id, dag_run_1.creating_job_id AS dag_run_1_creating_job_id, dag_run_1.external_trigger AS dag_run_1_external_trigger, dag_run_1.run_type AS dag_run_1_run_type, dag_run_1.conf AS dag_run_1_conf, dag_run_1.data_interval_start AS dag_run_1_data_interval_start, dag_run_1.data_interval_end AS dag_run_1_data_interval_end, dag_run_1.last_scheduling_decision AS dag_run_1_last_scheduling_decision, dag_run_1.dag_hash AS dag_run_1_dag_hash, dag_run_1.log_template_id AS dag_run_1_log_template_id 
FROM xcom JOIN dag_run ON xcom.dag_run_id = dag_run.id LEFT OUTER JOIN dag_run AS dag_run_1 ON xcom.dag_run_id = dag_run_1.id 
WHERE xcom.dag_id = %(dag_id_1)s AND dag_run.execution_date <= %(execution_date_1)s ORDER BY dag_run.execution_date DESC, xcom.timestamp DESC
[2022-11-06T17:36:56.861-0300] {taskinstance.py:1851} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/debian/etlhausz/venv/lib/python3.9/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/debian/etlhausz/venv/lib/python3.9/site-packages/airflow/operators/python.py", line 193, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/home/debian/etlhausz/airflow/dags/teste_excom.py", line 62, in _storing_models
    driver.get(lista)
  File "/home/debian/etlhausz/venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py", line 441, in get
    self.execute(Command.GET, {'url': url})
  File "/home/debian/etlhausz/venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py", line 427, in execute
    response = self.command_executor.execute(driver_command, params)
  File "/home/debian/etlhausz/venv/lib/python3.9/site-packages/selenium/webdriver/remote/remote_connection.py", line 342, in execute
    data = utils.dump_json(params)
  File "/home/debian/etlhausz/venv/lib/python3.9/site-packages/selenium/webdriver/remote/utils.py", line 23, in dump_json
    return json.dumps(json_struct)
  File "/usr/lib/python3.9/json/__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
  File "/usr/lib/python3.9/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/lib/python3.9/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/usr/lib/python3.9/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type BaseXCom is not JSON serializable
[2022-11-06T17:36:56.907-0300] {taskinstance.py:1401} INFO - Marking task as FAILED. dag_id=get_urls_deca, task_id=get_produtos, execution_date=20221106T202417, start_date=20221106T203655, end_date=20221106T203656
[2022-11-06T17:36:56.970-0300] {standard_task_runner.py:100} ERROR - Failed to execute job 420 for task get_produtos (Object of type BaseXCom is not JSON serializable; 3698)
[2022-11-06T17:36:57.005-0300] {local_task_job.py:164} INFO - Task exited with return code 1
[2022-11-06T17:36:57.125-0300] {local_task_job.py:273} INFO - 0 downstream tasks scheduled from follow-on schedule check
